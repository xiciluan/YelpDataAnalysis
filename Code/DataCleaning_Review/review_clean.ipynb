{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data cleaning**\n",
    "0. lower all the letters.\n",
    "1. change \"n't\" to \" not\".\n",
    "2. remove all the symbols except letters. \n",
    "3. remove all the stopwords. (should we keep the \"not sth\"?Yes, keep \"no\" and \"not\")\n",
    "4. normalize the verb and noun ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\yhlan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yhlan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\yhlan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the system not to print warnings\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.remove(\"no\")\n",
    "stop_words.remove(\"not\")\n",
    "stop_words.add(\"im\")\n",
    "stop_words.add(\"seem\")\n",
    "stop_words.add(\"u\")\n",
    "stop_words.add(\"ca\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv(\"review_select.csv\")\n",
    "reviews_text = reviews.loc[:,\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "def data_clean(review):\n",
    "    #lower all the letters\n",
    "    review = review.lower()\n",
    "    #change \"n't\" to \" not\"\n",
    "    review = re.sub(\"\\n\",\"\",review)\n",
    "    review = re.sub(\"n't\",\" not\",review)\n",
    "    review = re.sub(\"n'\",\" not\",review)\n",
    "    #remove all the symbols but letters\n",
    "    review = re.sub(\"[^a-zA-Z]\",\" \",review)\n",
    "    #remove all the stopwords except \"no\" and \"not\"\n",
    "    review_words = word_tokenize(review)\n",
    "    filtered_review = [w for w in review_words if not w in stop_words]\n",
    "    #normalize the verb and noun \n",
    "    normalized_review = [lemmatizer.lemmatize(w,\"v\") for w in filtered_review]\n",
    "    return \" \".join(normalized_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(reviews_text)):\n",
    "    reviews_text[i] = data_clean(reviews_text[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139988 85691 109784 205595 323479\n"
     ]
    }
   ],
   "source": [
    "stars = reviews.loc[:,\"stars\"]\n",
    "#print(sum(stars == 1.0),sum(stars == 2.0),sum(stars == 3.0),sum(stars == 4.0),sum(stars == 5.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save\n",
    "review_save=pd.DataFrame(reviews_text,columns=['text'])\n",
    "stars_save=pd.DataFrame(stars,columns=['stars'])\n",
    "\n",
    "data_save = pd.concat([review_save,stars_save],axis=1)\n",
    "\n",
    "data_save.to_csv('reviews_clean.csv', sep=',', header=True, index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
